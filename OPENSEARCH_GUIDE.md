# ğŸ§  **GuÃ­a de OpenSearch con Vector Store**

## ğŸ“‹ **Resumen**

Este proyecto ahora utiliza **OpenSearch** como vector store para implementar un sistema RAG (Retrieval-Augmented Generation) de alta precisiÃ³n. Los documentos se convierten en embeddings vectoriales y se almacenan en OpenSearch para bÃºsquedas semÃ¡nticas.

---

## ğŸ—ï¸ **Arquitectura del Sistema**

```
ğŸ“„ Documentos â†’ ğŸª£ S3 â†’ ğŸ”„ Lambda Processor â†’ ğŸ§  OpenSearch â†’ ğŸ¤– Chat Lambda â†’ ğŸ’¬ Respuesta
```

### **Componentes:**

1. **S3 Bucket**: Almacena documentos originales
2. **Lambda Processor**: Procesa documentos y crea embeddings
3. **OpenSearch**: Base de datos vectorial para bÃºsqueda semÃ¡ntica
4. **Chat Lambda**: Realiza bÃºsquedas vectoriales y genera respuestas

---

## ğŸš€ **Deployment**

### **1. Deployar la Infraestructura**
```bash
npm run deploy
```

### **2. Procesar Documentos Existentes**
```bash
# Obtener el nombre del bucket y endpoint de OpenSearch del output del CDK
npm run process-existing-documents <bucket-name> <opensearch-endpoint>
```

### **3. Subir Nuevos Documentos**
```bash
# Los documentos se procesan automÃ¡ticamente cuando se suben a S3
npm run upload-documents <bucket-name>
```

---

## ğŸ”§ **ConfiguraciÃ³n de OpenSearch**

### **Ãndice de Documentos**
```json
{
  "mappings": {
    "properties": {
      "content": {
        "type": "text",
        "analyzer": "standard"
      },
      "embedding": {
        "type": "dense_vector",
        "dims": 1536,
        "index": true,
        "similarity": "cosine"
      },
      "source": {
        "type": "keyword"
      },
      "chunk_index": {
        "type": "integer"
      },
      "timestamp": {
        "type": "date"
      }
    }
  }
}
```

### **BÃºsqueda Vectorial**
```json
{
  "size": 5,
  "query": {
    "script_score": {
      "query": { "match_all": {} },
      "script": {
        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
        "params": { "query_vector": [0.1, 0.2, ...] }
      }
    }
  }
}
```

---

## ğŸ“Š **Procesamiento de Documentos**

### **1. Chunking**
- Los documentos se dividen en chunks de ~1000 caracteres
- Se mantiene la integridad de las oraciones
- Cada chunk se procesa independientemente

### **2. Embeddings**
- Se usa el modelo `text-embedding-3-small` de OpenAI
- Dimensiones: 1536
- Similaridad: Cosine

### **3. IndexaciÃ³n**
- Cada chunk se indexa con su embedding
- ID Ãºnico: `{source}-{chunk_index}`
- Metadatos: fuente, Ã­ndice, timestamp

---

## ğŸ” **BÃºsqueda SemÃ¡ntica**

### **Flujo de BÃºsqueda:**
1. **Consulta del usuario** â†’ Embedding de la consulta
2. **BÃºsqueda vectorial** â†’ Chunks mÃ¡s similares
3. **Fallback** â†’ BÃºsqueda por palabras clave (si falla)
4. **Respuesta** â†’ Contexto + GeneraciÃ³n

### **Ventajas:**
- âœ… **BÃºsqueda semÃ¡ntica** (no solo palabras clave)
- âœ… **Mejor precisiÃ³n** en respuestas
- âœ… **Escalable** para grandes volÃºmenes
- âœ… **Fallback automÃ¡tico** si OpenSearch falla

---

## ğŸ’° **Costos Estimados**

### **OpenSearch:**
- **t3.small.search**: ~$30/mes
- **EBS 10GB**: ~$1/mes
- **Data Transfer**: ~$1-5/mes

### **Total OpenSearch:** ~$32-36/mes

### **Lambda (embeddings):**
- **Procesamiento**: ~$5-15/mes (dependiendo del volumen)

---

## ğŸ› ï¸ **Comandos Ãštiles**

### **Verificar OpenSearch**
```bash
# Verificar que el dominio estÃ¡ activo
aws opensearch list-domain-names

# Obtener endpoint
aws opensearch describe-domain --domain-name RAGDomain
```

### **Procesar Documentos**
```bash
# Procesar documentos existentes
npm run process-existing-documents <bucket> <endpoint>

# Subir nuevos documentos
npm run upload-documents <bucket>
```

### **Ver Logs**
```bash
# Logs del procesador de documentos
aws logs tail /aws/lambda/AiWebStack-DocumentProcessor

# Logs del chat
aws logs tail /aws/lambda/AiWebStack-ChatHandler
```

---

## ğŸ”§ **Troubleshooting**

### **Problema: OpenSearch no responde**
```bash
# Verificar estado del dominio
aws opensearch describe-domain --domain-name RAGDomain

# Verificar logs de Lambda
aws logs tail /aws/lambda/AiWebStack-DocumentProcessor
```

### **Problema: Embeddings no se crean**
```bash
# Verificar API key de OpenAI
npm run test-weather

# Verificar logs de procesamiento
aws logs tail /aws/lambda/AiWebStack-DocumentProcessor
```

### **Problema: BÃºsqueda lenta**
- Aumentar `dataNodeInstanceType` en CDK
- Optimizar tamaÃ±o de chunks
- Usar Ã­ndices mÃ¡s pequeÃ±os

---

## ğŸ“ˆ **MÃ©tricas de OpenSearch**

### **Ver estadÃ­sticas del Ã­ndice:**
```bash
curl -X GET "https://<opensearch-endpoint>/documents/_stats?pretty"
```

### **Ver documentos en OpenSearch:**
```bash
curl -X GET "https://<opensearch-endpoint>/documents/_search?pretty"
```

### **Ver mapping del Ã­ndice:**
```bash
curl -X GET "https://<opensearch-endpoint>/documents/_mapping?pretty"
```

---

## ğŸ”„ **Flujo AutomÃ¡tico**

### **Cuando se sube un documento:**
1. **S3 Event** â†’ Trigger Lambda Processor
2. **Lambda Processor** â†’ Crear embeddings
3. **OpenSearch** â†’ Indexar embeddings
4. **Chat** â†’ BÃºsqueda vectorial disponible

### **Cuando se hace una pregunta:**
1. **Consulta** â†’ Crear embedding
2. **OpenSearch** â†’ BÃºsqueda vectorial
3. **Resultados** â†’ Contexto para GPT
4. **Respuesta** â†’ GeneraciÃ³n con contexto

---

## ğŸ¯ **Beneficios del Vector Store**

### **Antes (BÃºsqueda Simple):**
- âŒ Solo palabras clave
- âŒ Sin contexto semÃ¡ntico
- âŒ Respuestas menos precisas

### **Ahora (Vector Store):**
- âœ… BÃºsqueda semÃ¡ntica
- âœ… Contexto rico
- âœ… Respuestas mÃ¡s precisas
- âœ… Escalabilidad

---

## ğŸš€ **PrÃ³ximos Pasos**

1. **Deployar** la nueva infraestructura
2. **Procesar** documentos existentes
3. **Probar** bÃºsquedas semÃ¡nticas
4. **Optimizar** segÃºn necesidades

Â¡Tu sistema RAG ahora tiene capacidades de bÃºsqueda semÃ¡ntica avanzada! ğŸ‰
